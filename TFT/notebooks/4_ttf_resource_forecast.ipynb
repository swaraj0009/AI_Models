{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import RMSE\n",
    "from pytorch_lightning import Trainer\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_parquet(\"/home/data/processed/FeatureEng.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Drop NA values for all required target variables\n",
    "tft_df = df3.dropna(subset=['cpu_utilization_ratio', 'memory_utilization_ratio', 'disk_total_throughput', 'network_total_throughput'])\n",
    "df3['vm_id'] = df3['VM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variables\n",
    "# targets = ['cpu_utilization_ratio', 'memory_utilization_ratio', 'disk_total_throughput', 'network_total_throughput']\n",
    "targets = ['cpu_utilization_ratio']\n",
    "time_varying_known_reals = ['time_idx', 'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos', 'month_sin', 'month_cos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import RMSE\n",
    "\n",
    "# üîß Unified config: change here only\n",
    "train_config = {\n",
    "    \"targets\": ['cpu_utilization_ratio'],  # change to full list if needed\n",
    "    \"time_varying_known_reals\": ['time_idx', 'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos', 'month_sin', 'month_cos'],\n",
    "    \"group_ids\": ['vm_id'],\n",
    "    \"max_encoder_length\": 8,\n",
    "    \"max_prediction_length\": 2,\n",
    "    \"hidden_size\": 4,\n",
    "    \"dropout\": 0.1,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"batch_size\": 4,\n",
    "    \"epochs\": 1,\n",
    "    \"loss_fn\": RMSE(),\n",
    "    \"output_base_dir\": \"/home/output\",\n",
    "    \"log_dir\": \"/home/output/logs\",\n",
    "    \"accelerator_opt\": \"cpu\",\n",
    "}\n",
    "\n",
    "# üöÄ Run for each target\n",
    "for target in train_config[\"targets\"]:\n",
    "    print(f\"\\nüîÅ Training for target: {target}\")\n",
    "\n",
    "    print(f\"üíæ Torch using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "    print(f\"üß† Total rows in full tft_df: {len(tft_df)}\")\n",
    "\n",
    "\n",
    "    run_dir = os.path.join(train_config[\"output_base_dir\"], f\"{target}_run\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    train_df = tft_df[tft_df.time_idx <= tft_df['time_idx'].max() * 0.8].copy()\n",
    "    train_df = train_df[np.isfinite(train_df[target])]\n",
    "    train_df = train_df.dropna(subset=[target])\n",
    "    bad_rows = tft_df[~np.isfinite(tft_df[target]) | tft_df[target].isna()]\n",
    "    print(f\"‚ö†Ô∏è {len(bad_rows)} bad rows removed for target: {target}\")\n",
    "\n",
    "    # Dataset\n",
    "    dataset = TimeSeriesDataSet(\n",
    "        train_df,\n",
    "        time_idx='time_idx',\n",
    "        target=target,\n",
    "        group_ids=train_config[\"group_ids\"],\n",
    "        max_encoder_length=train_config[\"max_encoder_length\"],\n",
    "        max_prediction_length=train_config[\"max_prediction_length\"],\n",
    "        time_varying_known_reals=train_config[\"time_varying_known_reals\"],\n",
    "        time_varying_unknown_reals=train_config[\"targets\"],\n",
    "        target_normalizer=GroupNormalizer(groups=train_config[\"group_ids\"]),\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "        allow_missing_timesteps=True\n",
    "    )\n",
    "\n",
    "    val_dataset = TimeSeriesDataSet.from_dataset(dataset, tft_df, predict=True, stop_randomization=True, allow_missing_timesteps=True)\n",
    "\n",
    "    train_dataloader = dataset.to_dataloader(train=True, batch_size=train_config[\"batch_size\"], num_workers=0)\n",
    "    val_dataloader = val_dataset.to_dataloader(train=False, batch_size=train_config[\"batch_size\"], num_workers=0)\n",
    "\n",
    "    # Logger and checkpoint\n",
    "    logger = CSVLogger(save_dir=train_config[\"log_dir\"], name=f\"{target}_log\")\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath=run_dir,\n",
    "        filename=\"tft-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=1,\n",
    "        save_last=True,\n",
    "        mode=\"min\"\n",
    "    )\n",
    "\n",
    "    # Load or create model\n",
    "    ckpt_path = os.path.join(run_dir, \"tft-last.ckpt\")\n",
    "    if os.path.exists(ckpt_path):\n",
    "        print(f\"üì¶ Resuming from checkpoint: {ckpt_path}\")\n",
    "        model = TemporalFusionTransformer.load_from_checkpoint(\n",
    "            checkpoint_path=ckpt_path,\n",
    "            dataset=dataset,\n",
    "            loss=train_config[\"loss_fn\"]\n",
    "        )\n",
    "    else:\n",
    "        print(\"üÜï Starting new model\")\n",
    "        model = TemporalFusionTransformer.from_dataset(\n",
    "            dataset,\n",
    "            learning_rate=train_config[\"learning_rate\"],\n",
    "            hidden_size=train_config[\"hidden_size\"],\n",
    "            dropout=train_config[\"dropout\"],\n",
    "            loss=train_config[\"loss_fn\"],\n",
    "            log_interval=10,\n",
    "            reduce_on_plateau_patience=4,\n",
    "        )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=train_config[\"epochs\"],\n",
    "        accelerator=[\"accelerator_opt\"],\n",
    "        devices=1 if torch.cuda.is_available() else None,\n",
    "        logger=logger,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        enable_checkpointing=True\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "\n",
    "    # Predict\n",
    "    predictions, x = model.predict(val_dataloader, mode='raw', return_x=True)\n",
    "    forecast = predictions['prediction'][0].detach().cpu().numpy()\n",
    "\n",
    "    # Plot and save\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    model.plot_prediction(x, predictions, idx=0, show_future_observed=True)\n",
    "    plt.title(f\"Prediction Plot for {target}\")\n",
    "    plt.savefig(f\"{run_dir}/plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save predictions\n",
    "    pd.DataFrame(forecast, columns=[f'{target}_forecast']).to_csv(f\"{run_dir}/predictions.csv\", index=False)\n",
    "\n",
    "    # Save loss log\n",
    "    log_csv_path = os.path.join(logger.log_dir, \"metrics.csv\")\n",
    "    if os.path.exists(log_csv_path):\n",
    "        shutil.copy(log_csv_path, f\"{run_dir}/loss_log.csv\")\n",
    "\n",
    "    # Save parameters\n",
    "    with open(f\"{run_dir}/params.json\", \"w\") as f:\n",
    "        json.dump(train_config, f, indent=2)\n",
    "\n",
    "    # Save notes with spikes info\n",
    "    spikes = forecast > np.percentile(forecast, 95)\n",
    "    with open(f\"{run_dir}/notes.txt\", \"w\") as f:\n",
    "        f.write(f\"Target: {target}\\n\")\n",
    "        f.write(f\"Spikes > 95th percentile: {int(spikes.sum())}\\n\")\n",
    "        f.write(\"Review plot.png and predictions.csv for further insights.\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Run complete ‚Äî outputs saved at: {run_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('forecasting_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2133a3972b28273c10fa027bbde5fb58efc69f3a1cd517826cf4b1affadfce4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
