{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swaraj0009/AI_Models/blob/master/TFT/notebooks/4_ttf_resource_forecast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drive Loading"
      ],
      "metadata": {
        "id": "Zut3EaByhFTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9imFCsDrwtVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "is7xTtSYgp0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ö° Quick Setup - Run after runtime reset (CPU/GPU Switch)\n",
        "# Installs essential packages silently to save output clutter\n",
        "\n",
        "!pip install dask pytz torch pytorch-forecasting pytorch-lightning \\\n",
        "    rich colorama matplotlib seaborn pandas numpy tensorboard \\\n",
        "    'lightning[extra]' pyarrow fastparquet --quiet > /dev/null\n",
        "\n",
        "print(\"\\033[92m‚úÖ All required packages installed successfully.\\033[0m\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "C9TAeg3MxRMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "MEIX0ouRhGwT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMD5usllwmtY"
      },
      "outputs": [],
      "source": [
        "# Standard Library\n",
        "import os\n",
        "import datetime\n",
        "import glob\n",
        "import json\n",
        "import shutil\n",
        "import math\n",
        "import pytz\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Third-Party Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "\n",
        "# PyTorch Lightning\n",
        "# from datetime import datetime\n",
        "import pytorch_forecasting\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "# PyTorch Forecasting\n",
        "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import RMSE\n",
        "from pytorch_forecasting.data import NaNLabelEncoder\n",
        "from torch.utils.data import DataLoader\n",
        "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
        "\n",
        "\n",
        "ist = pytz.timezone('Asia/Kolkata')\n",
        "now_ist = datetime.datetime.now(ist)\n",
        "timestamp = now_ist.strftime(\"%Y%m%d-%H%M%S\")\n",
        "print(f\"All Libraries are loaded : {timestamp}\")\n",
        "\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"‚ö° Using device: {device.upper()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User Configurable Parameters"
      ],
      "metadata": {
        "id": "YsYJkzavg7c5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Data path & VM selection\n",
        "parquet_path = \"/content/drive/MyDrive/datasets/processed/FeatureEng_full_streak.parquet\"\n",
        "\n",
        "# Model training parameters\n",
        "train_config = {\n",
        "    # üéØ Prediction Targets\n",
        "\n",
        "    # [\n",
        "    #     \"cpu_utilization_ratio\",\n",
        "    #     \"memory_utilization_ratio\",\n",
        "    #     \"disk_total_throughput\",\n",
        "    #     \"network_total_throughput\"\n",
        "    # ],\n",
        "\n",
        "    \"targets\": [\"cpu_utilization_ratio\"],\n",
        "\n",
        "    # üìÖ Known time-dependent features (known at prediction time)\n",
        "    \"time_varying_known_reals\": [\n",
        "        \"time_idx\",\n",
        "        \"hour\", \"day\", \"dayofweek\", \"month\", \"is_weekend\",\n",
        "        \"hour_sin\", \"hour_cos\",\n",
        "        \"dayofweek_sin\", \"dayofweek_cos\",\n",
        "        \"month_sin\", \"month_cos\"\n",
        "    ],\n",
        "\n",
        "    # üìà Features only known up to current timestep (future unknown)\n",
        "    \"time_varying_unknown_reals\": [\n",
        "        \"cpu_util_prev\", \"cpu_util_diff\",\n",
        "        \"memory_util_prev\", \"memory_util_diff\",\n",
        "        \"network_total_prev\", \"network_total_diff\",\n",
        "        \"disk_write_prev\", \"disk_write_diff\",\n",
        "        \"disk_rolling_mean\", \"network_rolling_mean\"\n",
        "    ],\n",
        "\n",
        "    # üîê Grouping feature\n",
        "    \"group_ids\": [\"vm_id\"],\n",
        "\n",
        "    # üß† Sequence lengths (adjust based on resources)\n",
        "    \"max_encoder_length\": 2100,      # input length\n",
        "    \"max_prediction_length\": 2016,     # forecast horizon\n",
        "\n",
        "    # ‚öôÔ∏è Model Hyperparameters (tune later)\n",
        "    \"hidden_size\": 128,\n",
        "    \"dropout\": 16,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"batch_size\": 16,\n",
        "    \"num_workers\": 3,\n",
        "\n",
        "    # üõë Early stopping\n",
        "    \"early_stopping_patience\": 5,\n",
        "    \"epochs\": 15,\n",
        "\n",
        "    # üßÆ Loss function\n",
        "    \"loss_fn\": RMSE(),\n",
        "\n",
        "    # üíæ Output paths\n",
        "    \"output_base_dir\": \"/content/drive/MyDrive/output\",\n",
        "    \"log_dir\": \"/content/drive/MyDrive/output/logs\"\n",
        "}"
      ],
      "metadata": {
        "id": "Y4lrYzgxwbLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5 = pd.read_parquet(parquet_path)\n",
        "\n",
        "print(f\"‚úÖ Loaded data shape: {df5.shape}\")\n",
        "print(f\"üî¢ Unique VMs: {df5['vm_id'].nunique()}\")"
      ],
      "metadata": {
        "id": "Fj81xFUJjf_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5[['time_idx', 'timestamp']].sort_values('time_idx').head(5)"
      ],
      "metadata": {
        "id": "9f6TvHiKzZxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5[['time_idx', 'timestamp']].sort_values('time_idx').tail(5)"
      ],
      "metadata": {
        "id": "T39kPwhKzuJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by VM and count how many time steps each VM has\n",
        "vm_streaks = df5.groupby(\"vm_id\").agg(\n",
        "    total_points=(\"time_idx\", \"count\"),\n",
        "    max_time_idx=(\"time_idx\", \"max\")\n",
        ").reset_index()\n",
        "\n",
        "# Sort by total_points (or max_time_idx) descending\n",
        "top_200_vms = vm_streaks.sort_values(by=\"total_points\", ascending=False).head(200)[\"vm_id\"]\n",
        "\n",
        "print(f\"‚úÖ Selected top 200 VMs with longest data streaks.\")"
      ],
      "metadata": {
        "id": "ybFhjPlCj2i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VM Configure"
      ],
      "metadata": {
        "id": "iIP9NMkMhW4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter original DataFrame for these 200 VMs\n",
        "df6 = df5[df5[\"vm_id\"].isin(top_200_vms)].copy()\n",
        "print(f\"‚úÖ Filtered data shape (top 200 VMs): {df6.shape}\")"
      ],
      "metadata": {
        "id": "6stdAQxokYhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"üéØ VMs in final dataset: {df6['vm_id'].nunique()}\")  # Should be 200"
      ],
      "metadata": {
        "id": "OglSlB95k1f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Columns Filter"
      ],
      "metadata": {
        "id": "rDSk5HS0iVcy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpDH8N3TwmtZ"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Drop unused columns based on train_config\n",
        "columns_to_keep = (\n",
        "    train_config[\"time_varying_known_reals\"]\n",
        "    + train_config[\"time_varying_unknown_reals\"]\n",
        "    + train_config[\"targets\"]\n",
        "    + train_config[\"group_ids\"]\n",
        "    + ['time_idx', 'timestamp']\n",
        ")\n",
        "\n",
        "# üîÅ Remove duplicates in case of overlaps\n",
        "columns_to_keep = list(set(columns_to_keep))\n",
        "\n",
        "# üìâ Filter DataFrame\n",
        "df6 = df6[columns_to_keep]\n",
        "\n",
        "print(f\"‚úÖ Columns after filtering: {len(df6.columns)}\")\n",
        "\n",
        "# üßº Optimize category column\n",
        "if \"vm_id\" in df6.columns:\n",
        "    df6[\"vm_id\"] = df6[\"vm_id\"].astype(\"category\")\n",
        "    df6[\"vm_id\"] = df6[\"vm_id\"].cat.remove_unused_categories()\n",
        "\n",
        "print(f\"\\033[94m‚ÑπÔ∏è Clean DataFrame ‚Üí Columns: {len(df6.columns)} | Shape: {df6.shape}\\033[0m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Logic with Reset Index"
      ],
      "metadata": {
        "id": "7lxANWba7IRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.51\n",
        "\n",
        "train_df_list = []\n",
        "val_df_list = []\n",
        "\n",
        "for vm_id, group in df6.groupby(\"vm_id\",observed=False):\n",
        "    group = group.sort_values(\"time_idx\")\n",
        "    split_idx = int(len(group) * train_ratio)\n",
        "\n",
        "    train_df_list.append(group.iloc[:split_idx])\n",
        "    val_df_list.append(group.iloc[split_idx:])\n",
        "\n",
        "# Combine all\n",
        "train_df = pd.concat(train_df_list).reset_index(drop=True)\n",
        "val_df = pd.concat(val_df_list).reset_index(drop=True)\n",
        "\n",
        "print(f\"‚úÖ Train shape: {train_df.shape}\")\n",
        "print(f\"‚úÖ Val shape: {val_df.shape}\")"
      ],
      "metadata": {
        "id": "pEgkRazj7DZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre validation check for split for Encoder & prediction"
      ],
      "metadata": {
        "id": "ZYOMUIIL7gi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define required steps\n",
        "required_train_steps = train_config[\"max_encoder_length\"] + train_config[\"max_prediction_length\"]\n",
        "required_val_steps = train_config[\"max_prediction_length\"]\n",
        "\n",
        "# Step 2: Get the split point\n",
        "max_time_idx = df6[\"time_idx\"].max()\n",
        "split_point = int(max_time_idx * train_ratio)\n",
        "\n",
        "# Step 3: Split data\n",
        "train_df = df6[df6[\"time_idx\"] <= split_point].copy()\n",
        "val_df = df6[df6[\"time_idx\"] > split_point].copy()\n",
        "\n",
        "# Step 4: Validate VMs having enough time points\n",
        "vm_train_counts = train_df.groupby(\"vm_id\",observed=False)[\"time_idx\"].nunique()\n",
        "vm_val_counts = val_df.groupby(\"vm_id\",observed=False)[\"time_idx\"].nunique()\n",
        "\n",
        "# Step 5: Filter valid VMs\n",
        "valid_train_vms = vm_train_counts[vm_train_counts >= required_train_steps].index\n",
        "valid_val_vms = vm_val_counts[vm_val_counts >= required_val_steps].index\n",
        "\n",
        "# Step 6: Filter DataFrames\n",
        "train_df = train_df[train_df[\"vm_id\"].isin(valid_train_vms)].copy()\n",
        "val_df = val_df[val_df[\"vm_id\"].isin(valid_val_vms)].copy()\n",
        "\n",
        "# Step 7: Summary\n",
        "print(\"\\nüìä VM-Level Split Window Check\\n\")\n",
        "print(f\"‚úÖ VMs valid for training   : {len(valid_train_vms)} / {vm_train_counts.shape[0]}\")\n",
        "print(f\"‚úÖ VMs valid for validation : {len(valid_val_vms)} / {vm_val_counts.shape[0]}\")"
      ],
      "metadata": {
        "id": "4fLQyI2hqd-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Configuration"
      ],
      "metadata": {
        "id": "opiXlao30zOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_encoder_length = train_config[\"max_encoder_length\"]\n",
        "max_prediction_length = train_config[\"max_prediction_length\"]\n",
        "group_ids = train_config[\"group_ids\"]\n",
        "targets = train_config[\"targets\"]  # e.g., [\"cpu_utilization_ratio\"]\n",
        "time_varying_known_reals = train_config[\"time_varying_known_reals\"]\n",
        "time_varying_unknown_reals = train_config[\"time_varying_unknown_reals\"]\n",
        "batchsize = train_config[\"batch_size\"]\n",
        "numworkers = train_config[\"num_workers\"]\n",
        "learningrate = train_config[\"learning_rate\"]\n",
        "hiddensize = train_config[\"hidden_size\"]\n",
        "lossfn = train_config[\"loss_fn\"]\n",
        "dropout = train_config[\"dropout\"]"
      ],
      "metadata": {
        "id": "MBHvjTvw0wNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TimeSeriesDataSet"
      ],
      "metadata": {
        "id": "zs0rxyuQuH1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Dataset\n",
        "tft_dataset = TimeSeriesDataSet(\n",
        "    train_df,\n",
        "    time_idx=\"time_idx\",\n",
        "    target=targets[0],  # Start with first target (e.g., \"cpu_utilization_ratio\")\n",
        "    group_ids=group_ids,\n",
        "    max_encoder_length=max_encoder_length,\n",
        "    max_prediction_length=max_prediction_length,\n",
        "    time_varying_known_reals=time_varying_known_reals,\n",
        "    time_varying_unknown_reals=time_varying_unknown_reals,\n",
        "    static_categoricals=[],\n",
        "    static_reals=[],\n",
        "    target_normalizer=GroupNormalizer(groups=group_ids),\n",
        "    add_relative_time_idx=True,\n",
        "    add_target_scales=True,\n",
        "    add_encoder_length=True,\n",
        ")\n",
        "\n",
        "val_dataset = tft_dataset.from_dataset(\n",
        "    tft_dataset,\n",
        "    val_df,\n",
        "    predict=True,\n",
        "    stop_randomization=True\n",
        ")\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = tft_dataset.to_dataloader(train=True, batch_size=batchsize, num_workers=numworkers)\n",
        "val_dataloader = val_dataset.to_dataloader(train=False, batch_size=batchsize, num_workers=numworkers)"
      ],
      "metadata": {
        "id": "RkrZt4FfzkVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output & Log Folder Creation"
      ],
      "metadata": {
        "id": "BoJ1pukZhskL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Short name for target\n",
        "def get_short_target_name(targets):\n",
        "    short_map = {\n",
        "        \"cpu_utilization_ratio\": \"cpu\",\n",
        "        \"memory_utilization_ratio\": \"mem\",\n",
        "        \"disk_total_throughput\": \"disk\",\n",
        "        \"network_total_throughput\": \"net\"\n",
        "    }\n",
        "    if isinstance(targets, list) and targets:\n",
        "        return short_map.get(targets[0], targets[0][:3])\n",
        "    return \"unknown\"\n",
        "\n",
        "# Step 2: Folder naming using extracted vars\n",
        "def get_run_folder_name(vm_count):\n",
        "    return \"_\".join([\n",
        "        get_short_target_name(targets),\n",
        "        f\"{vm_count}vms\",\n",
        "        f\"past{max_encoder_length}\",\n",
        "        f\"fut{max_prediction_length}\",\n",
        "        f\"bs{batchsize}\",\n",
        "        f\"lr{learningrate:.0e}\".replace('+0', ''),\n",
        "        f\"hid{hiddensize}\",\n",
        "        timestamp\n",
        "    ])\n",
        "\n",
        "# Step 3: Build folder name\n",
        "vm_count = df6[\"vm_id\"].nunique() # Calculate vm_count from df6\n",
        "folder_name = get_run_folder_name(vm_count)\n",
        "train_config[\"output_base_dir\"] = os.path.join(train_config[\"output_base_dir\"], folder_name)\n",
        "train_config[\"log_dir\"] = os.path.join(train_config[\"log_dir\"], folder_name)\n",
        "\n",
        "# Step 4: Create folders\n",
        "os.makedirs(train_config[\"output_base_dir\"], exist_ok=True)\n",
        "os.makedirs(train_config[\"log_dir\"], exist_ok=True)\n",
        "\n",
        "# Step 5: Print summary\n",
        "print(\"‚úÖ Output directory:\", train_config[\"output_base_dir\"])\n",
        "print(\"‚úÖ Log directory   :\", train_config[\"log_dir\"])"
      ],
      "metadata": {
        "id": "Ib5yqZQz-9PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tft_df = df6.copy()"
      ],
      "metadata": {
        "id": "uLNBdCq_YWb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logging & Callbacks"
      ],
      "metadata": {
        "id": "tLG5FXgbvkJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Use your existing function\n",
        "def get_short_target_name_single(target):\n",
        "    short_map = {\n",
        "        \"cpu_utilization_ratio\": \"cpu\",\n",
        "        \"memory_utilization_ratio\": \"mem\",\n",
        "        \"disk_total_throughput\": \"disk\",\n",
        "        \"network_total_throughput\": \"net\"\n",
        "    }\n",
        "    if isinstance(targets, list) and targets:\n",
        "        return short_map.get(targets[0], targets[0][:3])\n",
        "    return \"unknown\"\n",
        "\n",
        "for target in targets:\n",
        "    short_target = get_short_target_name_single(target)  # ‚úÖ Now uses each target in loop\n",
        "\n",
        "    print(f\"\\nüîÅ Training for target: {target}\")\n",
        "\n",
        "    run_dir = os.path.join(train_config[\"output_base_dir\"], f\"{short_target}_run_{timestamp}\")\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "    tft_df.to_csv(os.path.join(run_dir, \"tft_df.csv\"), index=False)\n",
        "    tft_dataset.save(os.path.join(run_dir, \"tft_df_metadata\"))\n",
        "\n",
        "    meta_cols = ['vm_id', 'timestamp', 'time_idx']\n",
        "    if all(col in val_df.columns for col in meta_cols):\n",
        "        meta_df = val_df[meta_cols].reset_index(drop=True)\n",
        "        meta_df.to_csv(os.path.join(run_dir, \"forecast_metadata.csv\"), index=False)\n",
        "        print(f\"‚úÖ Metadata saved to: {run_dir}/forecast_metadata.csv\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Skipping metadata save ‚Äî missing columns: {meta_cols}\")\n",
        "\n",
        "    logger = CSVLogger(\n",
        "        save_dir=train_config[\"log_dir\"],\n",
        "        name=f\"{short_target}_log\"  # ‚úÖ Each log is now uniquely named per target\n",
        "    )\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor=\"val_loss\",\n",
        "        dirpath=run_dir,\n",
        "        filename=\"tft-{epoch:02d}-{val_loss:.2f}\",\n",
        "        save_top_k=1,\n",
        "        save_last=True,\n",
        "        mode=\"min\"\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        patience=train_config[\"early_stopping_patience\"],\n",
        "        mode=\"min\"\n",
        "    )"
      ],
      "metadata": {
        "id": "X07NZKn0SeUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model, Lightning, Trainer"
      ],
      "metadata": {
        "id": "XuuT4XcivziV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "\n",
        "class TFTLightningModule(pl.LightningModule):\n",
        "    def __init__(self, tft_model: TemporalFusionTransformer, learning_rate: float, loss_fn: torch.nn.Module):\n",
        "        super().__init__()\n",
        "        self.tft_model = tft_model\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def setup(self, stage=None):  # ‚úÖ Add this\n",
        "        self.tft_model.to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.tft_model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.loss_fn(y_hat.prediction, y)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.loss_fn(y_hat.prediction, y)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "# Create the TFT model\n",
        "\n",
        "tft_model = TemporalFusionTransformer.from_dataset(\n",
        "    tft_dataset,\n",
        "    learning_rate=train_config[\"learning_rate\"],\n",
        "    hidden_size=train_config[\"hidden_size\"],\n",
        "    dropout=train_config[\"dropout\"],\n",
        "    loss=train_config[\"loss_fn\"],\n",
        "    log_interval=10,\n",
        "    reduce_on_plateau_patience=4\n",
        ")\n",
        "\n",
        "# Wrap the TFT model in a LightningModule\n",
        "model = TFTLightningModule(\n",
        "    tft_model=tft_model,\n",
        "    learning_rate=train_config[\"learning_rate\"],\n",
        "    loss_fn=train_config[\"loss_fn\"]\n",
        ")\n",
        "\n",
        "# Setup Trainer\n",
        "if torch.cuda.is_available():\n",
        "    accelerator = \"gpu\"\n",
        "    devices = 1\n",
        "else:\n",
        "    accelerator = \"cpu\"\n",
        "    devices = 1\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=train_config[\"epochs\"],\n",
        "    accelerator=accelerator,\n",
        "    devices=devices,\n",
        "    logger=logger,\n",
        "    callbacks=[checkpoint_callback, early_stopping],\n",
        "    enable_checkpointing=True\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
      ],
      "metadata": {
        "id": "Rqv7jTfcnEfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actual vs Prediction Graphs"
      ],
      "metadata": {
        "id": "YAkb7l6BytSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîÆ Step 1: Make raw predictions on validation set\n",
        "prediction_output = model.tft_model.predict(\n",
        "    val_dataloader, mode='raw', return_x=True\n",
        ")\n",
        "\n",
        "# ‚úÖ Step 2: Extract input and output\n",
        "x = prediction_output[\"x\"]\n",
        "predictions = prediction_output[\"output\"]\n",
        "\n",
        "# ‚úÖ Step 3: Extract forecast values as numpy array (for CSV export)\n",
        "forecast = predictions[\"prediction\"].detach().cpu().numpy()\n",
        "\n",
        "# ‚úÖ Step 4: Plot forecast using built-in TFT visualization\n",
        "fig = model.tft_model.plot_prediction(\n",
        "    x, predictions, idx=0, show_future_observed=True\n",
        ")\n",
        "plt.title(f\"Prediction Plot for {target}\")\n",
        "\n",
        "# ‚úÖ Reduce legend size and move it neatly outside\n",
        "plt.legend(\n",
        "    loc='upper left',\n",
        "    bbox_to_anchor=(1, 1),\n",
        "    fontsize='small',\n",
        "    frameon=True\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# ‚úÖ Step 5: Save the plot as PNG\n",
        "plt.savefig(f\"{run_dir}/plot.png\", bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"‚úÖ Prediction plot saved at: {run_dir}/plot.png\")\n",
        "\n",
        "# ‚úÖ Step 6: Save forecast to CSV\n",
        "pd.DataFrame(forecast, columns=[f'{target}_forecast']).to_csv(\n",
        "    f\"{run_dir}/predictions.csv\", index=False\n",
        ")\n",
        "print(f\"‚úÖ Forecast values saved to: {run_dir}/predictions.csv\")"
      ],
      "metadata": {
        "id": "B2qLHzHdywHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spike Detection & Save Metadata"
      ],
      "metadata": {
        "id": "Wp2UKawDwVRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Ensure forecast is a NumPy array\n",
        "if isinstance(forecast, torch.Tensor):\n",
        "    forecast = forecast.detach().cpu().numpy()\n",
        "elif isinstance(forecast, pd.Series):\n",
        "    forecast = forecast.values\n",
        "else:\n",
        "    forecast = np.array(forecast)\n",
        "\n",
        "# Detect spikes above the 95th percentile\n",
        "spikes = forecast > np.percentile(forecast, 95)\n",
        "spike_count = int(spikes.sum())\n",
        "\n",
        "# üîπ Save notes.txt with target name and spike info\n",
        "notes_path = os.path.join(run_dir, \"notes.txt\")\n",
        "with open(notes_path, \"w\") as f:\n",
        "    f.write(f\"Target: {target}\\n\")\n",
        "    f.write(f\"Spikes > 95th percentile: {spike_count}\\n\")\n",
        "    f.write(\"Review plot.png and predictions.csv for further insights.\\n\")\n",
        "\n",
        "print(f\"üìÑ Notes saved at: {notes_path}\")\n",
        "\n",
        "# üîπ Prepare train_config for JSON (remove non-serializable objects)\n",
        "serializable_train_config = train_config.copy()\n",
        "serializable_train_config[\"loss_fn\"] = serializable_train_config[\"loss_fn\"].__class__.__name__\n",
        "\n",
        "# üîπ Save model config as JSON\n",
        "config_path = os.path.join(run_dir, \"modelconfig.json\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    json.dump(serializable_train_config, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Config saved at: {config_path}\")\n",
        "print(f\"‚úÖ Run complete. Outputs saved at: {run_dir}\")\n",
        "\n",
        "forecast_path = os.path.join(run_dir, \"predictions.csv\")\n",
        "forecast_df.to_csv(forecast_path, index=False)\n",
        "print(f\"‚úÖ Forecast values saved to: {forecast_path}\")"
      ],
      "metadata": {
        "id": "ZZViKR3poBY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5iELq2nboDno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k9bYj41m4Dc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RmkEnl8nF--f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2133a3972b28273c10fa027bbde5fb58efc69f3a1cd517826cf4b1affadfce4e"
      }
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}