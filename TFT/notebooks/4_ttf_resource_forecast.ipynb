{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# prompt: write code for google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9imFCsDrwtVO",
        "outputId": "74c24139-208f-43a7-e4a1-13a3f3684288"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âš¡ Quick Setup - Run after runtime reset (CPU/GPU Switch)\n",
        "# Installs essential packages silently to save output clutter\n",
        "\n",
        "!pip install dask torch pytorch-forecasting pytorch-lightning \\\n",
        "    rich colorama matplotlib seaborn pandas numpy tensorboard \\\n",
        "    'lightning[extra]' pyarrow fastparquet --quiet\n",
        "\n",
        "print(\"\\033[92mâœ… All required packages installed successfully.\\033[0m\")"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9TAeg3MxRMV",
        "outputId": "ca46b678-1e4b-428b-9846-d4b3c48c9bb3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mâœ… All required packages installed successfully.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zMD5usllwmtY"
      },
      "outputs": [],
      "source": [
        "# Standard Library\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import shutil\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Third-Party Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "\n",
        "# PyTorch Lightning\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "# PyTorch Forecasting\n",
        "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import RMSE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "parquet_path = \"/content/drive/MyDrive/datasets/processed/FeatureEngcolab\"\n",
        "\n",
        "# List all VM partitions (folder names)\n",
        "vm_folders = sorted([\n",
        "    name.split('=')[1] for name in os.listdir(parquet_path) if name.startswith(\"VM=\")\n",
        "])\n",
        "\n",
        "print(f\"Available VMs: {vm_folders[:10]} ... Total: {len(vm_folders)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSFyV3GBVwiJ",
        "outputId": "5ebb5ba8-39a7-4290-ea0c-4880d738180f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available VMs: ['1', '10', '100', '1000', '1001', '1002', '1003', '1004', '1005', '1006'] ... Total: 1250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load First N VMs Dynamically [100, 250, 500, 750, 1000, 1250]\n",
        "\n",
        "for N in [30]:\n",
        "    selected_vms = vm_folders[:N]\n",
        "\n",
        "    df3 = dd.read_parquet(\n",
        "        parquet_path,\n",
        "        filters=[(\"VM\", \"in\", selected_vms)]\n",
        "    ).compute()\n",
        "\n",
        "    print(f\"âœ… Loaded {N} VMs â†’ Shape: {df3.shape}\")\n",
        "\n",
        "    # Optionally: Run model here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46RooB3uV2Ey",
        "outputId": "0786336b-54ec-49df-c2f3-b9cc20ab1199"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded 30 VMs â†’ Shape: (264533, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df3.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFFclZtb1XaB",
        "outputId": "4c34590d-2d6a-4002-9899-9a31fb66babc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Timestamp [s]', 'CPU cores', 'CPU capacity provisioned [MHZ]', 'CPU usage [MHZ]', 'CPU usage [%]', 'Memory capacity provisioned [KB]', 'Memory usage [KB]', 'Disk read throughput [KB/s]', 'Disk write throughput [KB/s]', 'Network received throughput [KB/s]', 'Network transmitted throughput [KB/s]', 'Timestamp', 'time_idx', 'time_diff', 'hour', 'dayofweek', 'is_weekend', 'month', 'day', 'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos', 'month_sin', 'month_cos', 'cpu_utilization_ratio', 'memory_utilization_ratio', 'cpu_util_percent', 'memory_util_percent', 'cpu_util_prev', 'cpu_util_diff', 'memory_util_prev', 'memory_util_diff', 'disk_total_throughput', 'disk_rolling_mean', 'disk_rolling_std', 'network_total_throughput', 'network_rolling_mean', 'network_rolling_std', 'disk_read_prev', 'disk_read_diff', 'disk_write_prev', 'disk_write_diff', 'network_received_prev', 'network_received_diff', 'network_transmitted_prev', 'network_transmitted_diff', 'network_total_prev', 'network_total_diff', 'VM']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df3[['Timestamp', 'time_idx']].tail())"
      ],
      "metadata": {
        "id": "ooBzXDblKrgM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JpDH8N3TwmtZ"
      },
      "outputs": [],
      "source": [
        "df3 = df3.rename(columns={'VM': 'vm_id'})\n",
        "\n",
        "tft_df = df3.dropna(subset=[\n",
        "    'cpu_utilization_ratio',\n",
        "    'memory_utilization_ratio',\n",
        "    'disk_total_throughput',\n",
        "    'network_total_throughput'\n",
        "])\n",
        "\n",
        "# Using Dask for big data preprocessing; switch to Pandas with .compute() as most ML models need Pandas DataFrame.\n",
        "\n",
        "tft_df = tft_df.compute() if 'dask' in str(type(tft_df)) else tft_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cy8HwIjFwmtZ"
      },
      "outputs": [],
      "source": [
        "# Define target variables\n",
        "# targets = ['cpu_utilization_ratio', 'memory_utilization_ratio', 'disk_total_throughput', 'network_total_throughput']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unified config â€” modify only here\n",
        "train_config = {\n",
        "    \"targets\": ['cpu_utilization_ratio'],\n",
        "    \"time_varying_known_reals\": ['time_idx', 'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos', 'month_sin', 'month_cos'],\n",
        "    \"group_ids\": ['vm_id'],\n",
        "    \"max_encoder_length\": 30,\n",
        "    \"max_prediction_length\": 6,\n",
        "    \"hidden_size\": 8,\n",
        "    \"dropout\": 0.1,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"batch_size\": 64,\n",
        "    \"num_workers\": 2,\n",
        "    \"epochs\": 2,\n",
        "    \"loss_fn\": RMSE(),\n",
        "    \"output_base_dir\": \"/content/drive/MyDrive/output\",\n",
        "    \"log_dir\": \"/content/drive/MyDrive/output/logs\"\n",
        "}\n",
        "\n",
        "# Ensure output folders exist\n",
        "os.makedirs(train_config[\"output_base_dir\"], exist_ok=True)\n",
        "os.makedirs(train_config[\"log_dir\"], exist_ok=True)"
      ],
      "metadata": {
        "id": "Ib5yqZQz-9PZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(tft_df['time_idx'].max())\n",
        "# print(tft_df[tft_df.time_idx > 6911])"
      ],
      "metadata": {
        "id": "fI3KS0f6F7sa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Min time_idx: {tft_df['time_idx'].min()}\")\n",
        "print(f\"Max time_idx: {tft_df['time_idx'].max()}\")\n",
        "print(f\"Total rows in tft_df: {len(tft_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNv3f4-azhzC",
        "outputId": "a95f7017-a18d-4ad9-d48a-201b7cfe0297"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min time_idx: 0\n",
            "Max time_idx: 8639\n",
            "Total rows in tft_df: 229536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Batch size used: {train_config['batch_size']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmu96UE00rqK",
        "outputId": "7f585192-6147-4581-fe93-1c1fee3a7fe7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size used: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_time_idx = tft_df['time_idx'].max()\n",
        "split_point = max_time_idx * 0.8\n",
        "\n",
        "print(f\"Max time_idx: {max_time_idx}\")\n",
        "print(f\"Split at time_idx > {split_point}\")\n",
        "\n",
        "train_rows = len(tft_df[tft_df.time_idx <= split_point])\n",
        "val_rows = len(tft_df[tft_df.time_idx > split_point])\n",
        "total_rows = len(tft_df)\n",
        "\n",
        "print(f\"Train rows: {train_rows}, Validation rows: {val_rows}, Total rows: {total_rows}\")\n",
        "print(f\"Val percentage: {100 * val_rows / total_rows:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh3z945cz7Mi",
        "outputId": "a57e10c4-3a60-44f9-cf16-4ad24cc8d106"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max time_idx: 8639\n",
            "Split at time_idx > 6911.200000000001\n",
            "Train rows: 186537, Validation rows: 42999, Total rows: 229536\n",
            "Val percentage: 18.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "print(f\"val_df rows: {len(val_df)}\")\n",
        "total_val_windows = len(val_df) - (train_config['max_encoder_length'] + train_config['max_prediction_length']) + 1\n",
        "print(f\"Total val windows: {total_val_windows}\")\n",
        "print(f\"Expected val batches: {math.ceil(total_val_windows / train_config['batch_size'])}\")\n",
        "\n",
        "# Print batch size to ensure no accidental override\n",
        "print(f\"Configured batch size: {train_config['batch_size']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVPKeiKE1Qv_",
        "outputId": "ce8bdee7-b6ec-4563-8898-51c030211dd9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_df rows: 42999\n",
            "Total val windows: 42964\n",
            "Expected val batches: 672\n",
            "Configured batch size: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_df = tft_df[tft_df.time_idx > tft_df['time_idx'].max() * 0.8]\n",
        "print(f\"Validation data points: {len(val_df)}\")\n",
        "print(f\"Minimum required: {train_config['max_encoder_length'] + train_config['max_prediction_length']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFHqfR9zSDck",
        "outputId": "998463c9-1446-43b2-98b1-0489c54a3f16"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation data points: 42999\n",
            "Minimum required: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset index (important for unique indexing)\n",
        "tft_df = tft_df.reset_index(drop=True)\n",
        "\n",
        "# Prepare TimeSeriesDataSet for training portion (80%)\n",
        "dataset = TimeSeriesDataSet(\n",
        "    tft_df[tft_df.time_idx <= tft_df['time_idx'].max() * 0.8],\n",
        "    time_idx='time_idx',\n",
        "    target=train_config[\"targets\"][0],  # 'cpu_utilization_ratio' here\n",
        "    group_ids=train_config[\"group_ids\"],\n",
        "    max_encoder_length=train_config[\"max_encoder_length\"],\n",
        "    max_prediction_length=train_config[\"max_prediction_length\"],\n",
        "    time_varying_known_reals=train_config[\"time_varying_known_reals\"],\n",
        "    time_varying_unknown_reals=train_config[\"targets\"],\n",
        "    target_normalizer=GroupNormalizer(groups=train_config[\"group_ids\"]),\n",
        "    add_relative_time_idx=True,\n",
        "    add_target_scales=True,\n",
        "    add_encoder_length=True,\n",
        "    allow_missing_timesteps=True\n",
        ")\n",
        "\n",
        "# Validation dataset for prediction (no randomization, full data)\n",
        "# âœ… \"TimeSeriesDataSet applies sliding window logic on the training data,\n",
        "# using the full configuration like past steps, future steps, groups, and prepares the dataset accordingly.\"\n",
        "\n",
        "val_df = val_df.reset_index(drop=True) # Reset index of val_df\n",
        "\n",
        "val_dataset = TimeSeriesDataSet.from_dataset(\n",
        "    dataset, val_df, predict=True, stop_randomization=True\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "\n",
        "train_dataloader = dataset.to_dataloader(\n",
        "    train=True,\n",
        "    batch_size=train_config[\"batch_size\"],\n",
        "    num_workers=train_config[\"num_workers\"]\n",
        ")\n",
        "\n",
        "val_dataloader = val_dataset.to_dataloader(\n",
        "    train=False,\n",
        "    batch_size=train_config[\"batch_size\"],\n",
        "    num_workers=train_config[\"num_workers\"]\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"âœ… Dataset and dataloaders ready. Train batches: {len(train_dataloader)}, Val batches: {len(val_dataloader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra7_fpzplKsf",
        "outputId": "06e792bc-d0df-4148-a339-94071246e30e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dataset and dataloaders ready. Train batches: 2898, Val batches: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "\n",
        "for target in train_config[\"targets\"]:\n",
        "    print(f\"\\nðŸ” Training for target: {target}\")\n",
        "\n",
        "    run_dir = os.path.join(train_config[\"output_base_dir\"], f\"{target}_run\")\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "    # Save cleaned dataset snapshot for debugging\n",
        "    tft_df.to_csv(f\"{run_dir}/tft_df.csv\", index=False)\n",
        "\n",
        "    # Setup logging & checkpointing\n",
        "    logger = CSVLogger(save_dir=train_config[\"log_dir\"], name=f\"{target}_log\")\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor=\"val_loss\",\n",
        "        dirpath=run_dir,\n",
        "        filename=\"tft-{epoch:02d}-{val_loss:.2f}\",\n",
        "        save_top_k=1,\n",
        "        save_last=True,\n",
        "        mode=\"min\"\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")"
      ],
      "metadata": {
        "id": "X07NZKn0SeUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "\n",
        "class TFTLightningModule(pl.LightningModule):\n",
        "    def __init__(self, tft_model: TemporalFusionTransformer, learning_rate: float, loss_fn: torch.nn.Module):\n",
        "        super().__init__()\n",
        "        self.tft_model = tft_model\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.tft_model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.loss_fn(y_hat.prediction, y) # Extract prediction from output\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.loss_fn(y_hat.prediction, y) # Extract prediction from output\n",
        "        self.log(\"val_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "# Create the TFT model\n",
        "tft_model = TemporalFusionTransformer.from_dataset(\n",
        "    dataset,\n",
        "    learning_rate=train_config[\"learning_rate\"],\n",
        "    hidden_size=train_config[\"hidden_size\"],\n",
        "    dropout=train_config[\"dropout\"],\n",
        "    loss=train_config[\"loss_fn\"],\n",
        "    log_interval=10,\n",
        "    reduce_on_plateau_patience=4\n",
        ")\n",
        "\n",
        "# Wrap the TFT model in a LightningModule\n",
        "model = TFTLightningModule(\n",
        "    tft_model=tft_model,\n",
        "    learning_rate=train_config[\"learning_rate\"],\n",
        "    loss_fn=train_config[\"loss_fn\"]\n",
        ")\n",
        "\n",
        "# Setup Trainer\n",
        "if torch.cuda.is_available():\n",
        "    accelerator = \"gpu\"\n",
        "    devices = 1\n",
        "else:\n",
        "    accelerator = \"cpu\"\n",
        "    devices = 1\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=train_config[\"epochs\"],\n",
        "    accelerator=accelerator,\n",
        "    devices=devices,\n",
        "    logger=logger,\n",
        "    callbacks=[checkpoint_callback, early_stopping],\n",
        "    enable_checkpointing=True\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
      ],
      "metadata": {
        "id": "paex9VsEaJBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## âœ… Chunk 5 â€” Prediction, Visualization, Forecast Saving"
      ],
      "metadata": {
        "id": "-HHkdXUKn61p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ” Make raw predictions on validation set\n",
        "predictions, x = model.tft_model.predict(val_dataloader, mode='raw', return_x=True)\n",
        "\n",
        "# Extract forecast values as numpy array\n",
        "forecast = predictions['prediction'][0].detach().cpu().numpy()\n",
        "\n",
        "# ðŸ“ˆ Plot forecast using built-in visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "model.tft_model.plot_prediction(x, predictions, idx=0, show_future_observed=True)\n",
        "plt.title(f\"Prediction Plot for {target}\")\n",
        "plt.savefig(f\"{run_dir}/plot.png\")\n",
        "plt.close()\n",
        "\n",
        "print(f\"âœ… Prediction plot saved at: {run_dir}/plot.png\")\n",
        "\n",
        "# ðŸ’¾ Save forecast to CSV\n",
        "pd.DataFrame(forecast, columns=[f'{target}_forecast']).to_csv(f\"{run_dir}/predictions.csv\", index=False)\n",
        "print(f\"âœ… Forecast values saved to: {run_dir}/predictions.csv\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ytkT1w0WbEx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9c50a474"
      },
      "source": [
        "# ðŸ” Make raw predictions on validation set\n",
        "prediction_output = model.tft_model.predict(val_dataloader, mode='raw', return_x=True)\n",
        "\n",
        "# Access predictions and x from the Prediction object\n",
        "predictions = prediction_output.output # Access the output attribute which contains the prediction tensor\n",
        "# The input batch 'x' is also contained within the Prediction object, often accessible directly or via an attribute\n",
        "# Let's assume 'x' is directly accessible as an attribute for now, if not, we may need to inspect the object further.\n",
        "# Based on the traceback, x is likely needed for plot_prediction. Let's try accessing it directly.\n",
        "x = prediction_output.x # Assuming x is an attribute of the Prediction object\n",
        "\n",
        "# Extract forecast values as numpy array\n",
        "forecast = predictions.prediction[0].detach().cpu().numpy() # Access the 'prediction' attribute of the output\n",
        "\n",
        "# ðŸ“ˆ Plot forecast using built-in visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "# The plot_prediction method expects the original input x and the predictions dictionary\n",
        "# We need to reconstruct a predictions dictionary similar to what was expected before\n",
        "predictions_dict = {'prediction': predictions.prediction} # Create a dictionary with the prediction tensor\n",
        "\n",
        "model.tft_model.plot_prediction(x, predictions_dict, idx=0, show_future_observed=True)\n",
        "plt.title(f\"Prediction Plot for {target}\")\n",
        "plt.savefig(f\"{run_dir}/plot.png\")\n",
        "plt.close()\n",
        "\n",
        "print(f\"âœ… Prediction plot saved at: {run_dir}/plot.png\")\n",
        "\n",
        "# ðŸ’¾ Save forecast to CSV\n",
        "pd.DataFrame(forecast, columns=[f'{target}_forecast']).to_csv(f\"{run_dir}/predictions.csv\", index=False)\n",
        "print(f\"âœ… Forecast values saved to: {run_dir}/predictions.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## âœ… Chunk 6 â€” Spike Detection & Save Metadata"
      ],
      "metadata": {
        "id": "Gn6zSuWTnP7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ” Simple spike detection based on 95th percentile threshold\n",
        "spikes = forecast > np.percentile(forecast, 95)\n",
        "\n",
        "# Save run notes and spike count\n",
        "with open(f\"{run_dir}/notes.txt\", \"w\") as f:\n",
        "    f.write(f\"Target: {target}\\n\")\n",
        "    f.write(f\"Spikes > 95th percentile: {int(spikes.sum())}\\n\")\n",
        "    f.write(\"Review plot.png and predictions.csv for further insights.\\n\")\n",
        "\n",
        "# ðŸ’¾ Save training config for reproducibility\n",
        "# Create a serializable version of train_config\n",
        "serializable_train_config = train_config.copy()\n",
        "# Replace the non-serializable loss_fn object with its name\n",
        "serializable_train_config[\"loss_fn\"] = serializable_train_config[\"loss_fn\"].__class__.__name__\n",
        "\n",
        "with open(f\"{run_dir}/params.json\", \"w\") as f:\n",
        "    json.dump(serializable_train_config, f, indent=2)\n",
        "\n",
        "print(f\"âœ… Run complete. Outputs saved at: {run_dir}\")"
      ],
      "metadata": {
        "id": "ZZViKR3poBY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5iELq2nboDno"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2133a3972b28273c10fa027bbde5fb58efc69f3a1cd517826cf4b1affadfce4e"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}