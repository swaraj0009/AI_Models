{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# prompt: write code for google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9imFCsDrwtVO",
        "outputId": "98cc4434-bae9-4c69-805f-f2723d5a1470"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Install Required Packages\n",
        "# !pip install torch pytorch-forecasting pytorch-lightning rich colorama matplotlib seaborn pandas numpy tensorboard lightning[extra] pyarrow fastparquet"
      ],
      "metadata": {
        "collapsed": true,
        "id": "C9TAeg3MxRMV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zMD5usllwmtY"
      },
      "outputs": [],
      "source": [
        "# Standard Library\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import shutil\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Third-Party Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "\n",
        "# PyTorch Lightning\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "# PyTorch Forecasting\n",
        "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import RMSE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "parquet_path = \"/content/drive/MyDrive/datasets/processed/FeatureEngcolab\"\n",
        "\n",
        "# List all VM partitions (folder names)\n",
        "vm_folders = sorted([\n",
        "    name.split('=')[1] for name in os.listdir(parquet_path) if name.startswith(\"VM=\")\n",
        "])\n",
        "\n",
        "print(f\"Available VMs: {vm_folders[:10]} ... Total: {len(vm_folders)}\")"
      ],
      "metadata": {
        "id": "kSFyV3GBVwiJ",
        "outputId": "f38c2e65-5025-4e02-e7ad-ddbd5491464e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available VMs: ['1', '10', '100', '1000', '1001', '1002', '1003', '1004', '1005', '1006'] ... Total: 1250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load First N VMs Dynamically [100, 250, 500, 750, 1000, 1250]\n",
        "\n",
        "for N in [50]:\n",
        "    selected_vms = vm_folders[:N]\n",
        "\n",
        "    df3 = dd.read_parquet(\n",
        "        parquet_path,\n",
        "        filters=[(\"VM\", \"in\", selected_vms)]\n",
        "    ).compute()\n",
        "\n",
        "    print(f\"‚úÖ Loaded {N} VMs ‚Üí Shape: {df3.shape}\")\n",
        "\n",
        "    # Optionally: Run model here"
      ],
      "metadata": {
        "id": "46RooB3uV2Ey",
        "outputId": "5dd0349a-4e7d-463e-b174-9fe354d8448c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 50 VMs ‚Üí Shape: (441151, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df3.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFFclZtb1XaB",
        "outputId": "e62576c8-cedd-4beb-a825-3454f8b69bdc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Timestamp [s]', 'CPU cores', 'CPU capacity provisioned [MHZ]', 'CPU usage [MHZ]', 'CPU usage [%]', 'Memory capacity provisioned [KB]', 'Memory usage [KB]', 'Disk read throughput [KB/s]', 'Disk write throughput [KB/s]', 'Network received throughput [KB/s]', 'Network transmitted throughput [KB/s]', 'VM', 'Timestamp', 'time_idx', 'time_diff', 'hour', 'dayofweek', 'is_weekend', 'month', 'day', 'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos', 'month_sin', 'month_cos', 'cpu_utilization_ratio', 'memory_utilization_ratio', 'cpu_util_percent', 'memory_util_percent', 'cpu_util_prev', 'cpu_util_diff', 'memory_util_prev', 'memory_util_diff', 'disk_total_throughput', 'disk_rolling_mean', 'disk_rolling_std', 'network_total_throughput', 'network_rolling_mean', 'network_rolling_std', 'disk_read_prev', 'disk_read_diff', 'disk_write_prev', 'disk_write_diff', 'network_received_prev', 'network_received_diff', 'network_transmitted_prev', 'network_transmitted_diff', 'network_total_prev', 'network_total_diff']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df3.rename(columns={'VM': 'vm_id'})"
      ],
      "metadata": {
        "id": "qBhWeCT44vGY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JpDH8N3TwmtZ"
      },
      "outputs": [],
      "source": [
        "tft_df = df3.dropna(subset=[\n",
        "    'cpu_utilization_ratio',\n",
        "    'memory_utilization_ratio',\n",
        "    'disk_total_throughput',\n",
        "    'network_total_throughput'\n",
        "]).compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cy8HwIjFwmtZ"
      },
      "outputs": [],
      "source": [
        "# Define target variables\n",
        "# targets = ['cpu_utilization_ratio', 'memory_utilization_ratio', 'disk_total_throughput', 'network_total_throughput']\n",
        "\n",
        "targets = ['cpu_utilization_ratio']\n",
        "time_varying_known_reals = [\n",
        "    'time_idx',\n",
        "    'hour_sin', 'hour_cos',\n",
        "    'dayofweek_sin', 'dayofweek_cos',\n",
        "    'month_sin', 'month_cos'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß Step 4: Unified config ‚Äî update only here\n",
        "train_config = {\n",
        "    \"targets\": ['cpu_utilization_ratio'],\n",
        "    \"time_varying_known_reals\": ['time_idx', 'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos', 'month_sin', 'month_cos'],\n",
        "    \"group_ids\": ['vm_id'],\n",
        "    \"max_encoder_length\": 12,\n",
        "    \"max_prediction_length\": 3,\n",
        "    \"hidden_size\": 8,\n",
        "    \"dropout\": 0.1,\n",
        "    \"learning_rate\": 0.03,\n",
        "    \"batch_size\": 16,\n",
        "    \"epochs\": 2,\n",
        "    \"loss_fn\": RMSE(),\n",
        "    \"output_base_dir\": \"/home/output\",\n",
        "    \"log_dir\": \"/home/output/logs\"\n",
        "}\n",
        "\n",
        "# üöÄ Step 5: Train for each target\n",
        "for target in train_config[\"targets\"]:\n",
        "    print(f\"\\nüîÅ Training for target: {target}\")\n",
        "\n",
        "    run_dir = os.path.join(train_config[\"output_base_dir\"], f\"{target}_run\")\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "    # üíæ Optional: Save cleaned df snapshot\n",
        "    tft_df.to_csv(f\"{run_dir}/tft_df.csv\", index=False)\n",
        "\n",
        "    # üß™ Dataset preparation\n",
        "    dataset = TimeSeriesDataSet(\n",
        "        tft_df[tft_df.time_idx <= tft_df['time_idx'].max() * 0.8],\n",
        "        time_idx='time_idx',\n",
        "        target=target,\n",
        "        group_ids=train_config[\"group_ids\"],\n",
        "        max_encoder_length=train_config[\"max_encoder_length\"],\n",
        "        max_prediction_length=train_config[\"max_prediction_length\"],\n",
        "        time_varying_known_reals=train_config[\"time_varying_known_reals\"],\n",
        "        time_varying_unknown_reals=train_config[\"targets\"],\n",
        "        target_normalizer=GroupNormalizer(groups=train_config[\"group_ids\"]),\n",
        "        add_relative_time_idx=True,\n",
        "        add_target_scales=True,\n",
        "        add_encoder_length=True,\n",
        "        allow_missing_timesteps=True\n",
        "    )\n",
        "\n",
        "    val_dataset = TimeSeriesDataSet.from_dataset(dataset, tft_df, predict=True, stop_randomization=True)\n",
        "\n",
        "    train_dataloader = dataset.to_dataloader(train=True, batch_size=train_config[\"batch_size\"], num_workers=0)\n",
        "    val_dataloader = val_dataset.to_dataloader(train=False, batch_size=train_config[\"batch_size\"], num_workers=0)\n",
        "\n",
        "    # üìä Logging and checkpoints\n",
        "    logger = CSVLogger(save_dir=train_config[\"log_dir\"], name=f\"{target}_log\")\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor=\"val_loss\",\n",
        "        dirpath=run_dir,\n",
        "        filename=\"tft-{epoch:02d}-{val_loss:.2f}\",\n",
        "        save_top_k=1,\n",
        "        save_last=True,\n",
        "        mode=\"min\"\n",
        "    )\n",
        "\n",
        "    # üîÅ Resume or initialize model\n",
        "    ckpt_path = os.path.join(run_dir, \"tft-last.ckpt\")\n",
        "    if os.path.exists(ckpt_path):\n",
        "        print(f\"üì¶ Resuming from checkpoint: {ckpt_path}\")\n",
        "        model = TemporalFusionTransformer.load_from_checkpoint(\n",
        "            checkpoint_path=ckpt_path,\n",
        "            dataset=dataset,\n",
        "            loss=train_config[\"loss_fn\"]\n",
        "        )\n",
        "    else:\n",
        "        print(\"üÜï Starting new model\")\n",
        "        model = TemporalFusionTransformer.from_dataset(\n",
        "            dataset,\n",
        "            learning_rate=train_config[\"learning_rate\"],\n",
        "            hidden_size=train_config[\"hidden_size\"],\n",
        "            dropout=train_config[\"dropout\"],\n",
        "            loss=train_config[\"loss_fn\"],\n",
        "            log_interval=10,\n",
        "            reduce_on_plateau_patience=4,\n",
        "        )\n",
        "\n",
        "    # üèãÔ∏è Trainer\n",
        "    trainer = Trainer(\n",
        "        max_epochs=train_config[\"epochs\"],\n",
        "        accelerator='auto',\n",
        "        devices=1 if torch.cuda.is_available() else None,\n",
        "        logger=logger,\n",
        "        callbacks=[checkpoint_callback],\n",
        "        enable_checkpointing=True\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
        "\n",
        "    # üîç Predict\n",
        "    predictions, x = model.predict(val_dataloader, mode='raw', return_x=True)\n",
        "    forecast = predictions['prediction'][0].detach().cpu().numpy()\n",
        "\n",
        "    # üìà Plot forecast\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    model.plot_prediction(x, predictions, idx=0, show_future_observed=True)\n",
        "    plt.title(f\"Prediction Plot for {target}\")\n",
        "    plt.savefig(f\"{run_dir}/plot.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # üíæ Save forecast data\n",
        "    pd.DataFrame(forecast, columns=[f'{target}_forecast']).to_csv(f\"{run_dir}/predictions.csv\", index=False)\n",
        "\n",
        "    # üíæ Save training log\n",
        "    log_csv_path = os.path.join(logger.log_dir, \"metrics.csv\")\n",
        "    if os.path.exists(log_csv_path):\n",
        "        shutil.copy(log_csv_path, f\"{run_dir}/loss_log.csv\")\n",
        "\n",
        "    # üíæ Save training config\n",
        "    with open(f\"{run_dir}/params.json\", \"w\") as f:\n",
        "        json.dump(train_config, f, indent=2)\n",
        "\n",
        "    # üìå Save spike info\n",
        "    spikes = forecast > np.percentile(forecast, 95)\n",
        "    with open(f\"{run_dir}/notes.txt\", \"w\") as f:\n",
        "        f.write(f\"Target: {target}\\n\")\n",
        "        f.write(f\"Spikes > 95th percentile: {int(spikes.sum())}\\n\")\n",
        "        f.write(\"Review plot.png and predictions.csv for further insights.\\n\")\n",
        "\n",
        "    print(f\"‚úÖ Run complete ‚Äî outputs saved at: {run_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib5yqZQz-9PZ",
        "outputId": "facfd25f-5594-48e0-c6be-a0307eac5a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÅ Training for target: cpu_utilization_ratio\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.12 ('forecasting_env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2133a3972b28273c10fa027bbde5fb58efc69f3a1cd517826cf4b1affadfce4e"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}