{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1"
      ],
      "metadata": {
        "id": "k7HI3qeIiOaq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write code for google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9imFCsDrwtVO",
        "outputId": "ccdb7ed7-0960-4b7b-bfcb-bccc393a3a28"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ö° Quick Setup - Run after runtime reset (CPU/GPU Switch)\n",
        "# Installs essential packages silently to save output clutter\n",
        "\n",
        "!pip install dask pytz torch pytorch-forecasting pytorch-lightning \\\n",
        "    rich colorama matplotlib seaborn pandas numpy tensorboard \\\n",
        "    'lightning[extra]' pyarrow fastparquet --quiet\n",
        "\n",
        "print(\"\\033[92m‚úÖ All required packages installed successfully.\\033[0m\")"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9TAeg3MxRMV",
        "outputId": "cdf6e2c0-d6e2-43db-cc6f-8938dd9b825e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_forecasting\n",
        "print(pytorch_forecasting.__version__)"
      ],
      "metadata": {
        "id": "FaIEEmvVFzpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMD5usllwmtY"
      },
      "outputs": [],
      "source": [
        "# Standard Library\n",
        "import os\n",
        "import datetime\n",
        "import glob\n",
        "import json\n",
        "import shutil\n",
        "import math\n",
        "import pytz\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Third-Party Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "\n",
        "# PyTorch Lightning\n",
        "# from datetime import datetime\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "# PyTorch Forecasting\n",
        "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import RMSE\n",
        "\n",
        "ist = pytz.timezone('Asia/Kolkata')\n",
        "now_ist = datetime.datetime.now(ist)\n",
        "timestamp = now_ist.strftime(\"%Y%m%d-%H%M%S\")\n",
        "print(f\"All Libraries are loaded : {timestamp}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- Main Processing ---------\n",
        "\n",
        "vm_folders = sorted([\n",
        "    name.split('=')[1] for name in os.listdir(parquet_path) if name.startswith(\"VM=\")\n",
        "])\n",
        "\n",
        "print(f\"Available VMs: {vm_folders[:10]} ... Total: {len(vm_folders)}\")\n",
        "\n",
        "# Load First N VMs Dynamically [100, 250, 500, 750, 1000, 1250]\n",
        "\n",
        "for N in [num_vms_to_load]:\n",
        "    selected_vms = vm_folders[:N]\n",
        "\n",
        "    df3 = dd.read_parquet(\n",
        "        parquet_path,\n",
        "        filters=[(\"VM\", \"in\", selected_vms)]\n",
        "    ).compute()\n",
        "\n",
        "    print(f\"‚úÖ Loaded {N} VMs ‚Üí Shape: {df3.shape}\")\n",
        "\n",
        "# print(df3.columns.tolist())\n",
        "# print(df3[['Timestamp', 'time_idx']].tail())\n",
        "\n",
        "# --- Column Change ---\n",
        "df3 = df3.rename(columns={'VM': 'vm_id'})\n",
        "\n",
        "tft_df = df3.dropna(subset=[\n",
        "    'cpu_utilization_ratio',\n",
        "    'memory_utilization_ratio',\n",
        "    'disk_total_throughput',\n",
        "    'network_total_throughput'\n",
        "])\n",
        "\n",
        "# Convert Dask to Pandas\n",
        "tft_df = tft_df.compute() if 'dask' in str(type(tft_df)) else tft_df\n",
        "\n",
        "print(f\"üìä Columns available before filtering: {len(tft_df.columns)}\")\n",
        "print(tft_df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "kSFyV3GBVwiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Dynamic Config with Dataset Size Labels ------------------\n",
        "\n",
        "import multiprocessing\n",
        "\n",
        "# Automatically detect how many CPU cores are available\n",
        "num_cpu_cores = multiprocessing.cpu_count()\n",
        "\n",
        "# Heuristic rule: use half the cores, but keep between 2 and 8\n",
        "dynamic_num_workers = min(max(num_cpu_cores // 2, 2), 8)\n",
        "\n",
        "# Update in training config\n",
        "train_config[\"num_workers\"] = dynamic_num_workers\n",
        "\n",
        "print(f\"‚úÖ Using {dynamic_num_workers} workers for DataLoader (detected {num_cpu_cores} cores)\")\n",
        "\n",
        "# Calculate dataset scale\n",
        "current_vm_rows = len(tft_df)\n",
        "vm_group = tft_df.groupby(\"vm_id\")\n",
        "rows_per_vm = vm_group.size().mean()\n",
        "\n",
        "# Label the dataset size\n",
        "if current_vm_rows < 200_000:\n",
        "    data_scale_label = \"SMALL\"\n",
        "    lr = 0.01\n",
        "    hidden = 8\n",
        "    dropout = 0.3\n",
        "elif current_vm_rows < 1_000_000:\n",
        "    data_scale_label = \"MEDIUM\"\n",
        "    lr = 0.005\n",
        "    hidden = 16\n",
        "    dropout = 0.2\n",
        "elif current_vm_rows < 5_000_000:\n",
        "    data_scale_label = \"LARGE\"\n",
        "    lr = 0.003\n",
        "    hidden = 32\n",
        "    dropout = 0.1\n",
        "else:\n",
        "    data_scale_label = \"XLARGE\"\n",
        "    lr = 0.002\n",
        "    hidden = 64\n",
        "    dropout = 0.1\n",
        "\n",
        "# Encoder length\n",
        "def get_encoder_length(row_count):\n",
        "    if row_count >= 5000:\n",
        "        return 400\n",
        "    elif row_count >= 3000:\n",
        "        return 300\n",
        "    elif row_count >= 2000:\n",
        "        return 200\n",
        "    elif row_count >= 1000:\n",
        "        return 100\n",
        "    elif row_count >= 600:\n",
        "        return 60\n",
        "    else:\n",
        "        return 30\n",
        "\n",
        "dynamic_encoder_length = get_encoder_length(rows_per_vm)\n",
        "\n",
        "# Batch size heuristic\n",
        "estimated_batch_size = int(min(max(0.001 * current_vm_rows, 16), 128))\n",
        "\n",
        "# Print final info\n",
        "print(f\"\\nüì¶ Dataset Scale: {data_scale_label}\")\n",
        "print(f\"üìä Total rows: {current_vm_rows}, Avg rows/VM: {rows_per_vm:.0f}\")\n",
        "print(f\"‚úÖ Encoder Length: {dynamic_encoder_length}\")\n",
        "print(f\"‚úÖ Config -> Batch: {estimated_batch_size}, LR: {lr}, Hidden: {hidden}, Dropout: {dropout}\")"
      ],
      "metadata": {
        "id": "o63krRCw9OYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- User Configurable Parameters ---------\n",
        "\n",
        "# Data path & VM selection\n",
        "parquet_path = \"/content/drive/MyDrive/datasets/processed/FeatureEngcolab\"\n",
        "\n",
        "# e.g., 50, 100, 250 etc.\n",
        "num_vms_to_load = 30\n",
        "\n",
        "# Model training parameters\n",
        "train_config = {\n",
        "    \"targets\": ['cpu_utilization_ratio'],\n",
        "    \"time_varying_known_reals\": ['time_idx', 'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos', 'month_sin', 'month_cos'],\n",
        "    \"time_varying_unknown_reals\": ['CPU usage [%]','Memory usage [KB]',\n",
        "    'Disk read throughput [KB/s]', 'Disk write throughput [KB/s]','Network received throughput [KB/s]', 'Network transmitted throughput [KB/s]',\n",
        "    'cpu_util_percent', 'memory_util_percent','disk_rolling_mean', 'disk_rolling_std','network_rolling_mean', 'network_rolling_std'],\n",
        "    \"group_ids\": ['vm_id'],\n",
        "    \"max_encoder_length\": dynamic_encoder_length,\n",
        "    \"max_preditction_length\": 2016,\n",
        "    \"hidden_size\": hidden,\n",
        "    \"dropout\": dropout,\n",
        "    \"learning_rate\": lr,\n",
        "    \"batch_size\": estimated_batch_size,\n",
        "    \"num_workers\": dynamic_num_workers,\n",
        "    \"early_stopping_patience\": 3,\n",
        "    \"epochs\": 1,\n",
        "    \"loss_fn\": RMSE(),\n",
        "    \"output_base_dir\": \"/content/drive/MyDrive/output\",\n",
        "    \"log_dir\": \"/content/drive/MyDrive/output/logs\"\n",
        "}\n",
        "\n",
        "# VM count for folder naming (update if needed)\n",
        "vm_count = f\"{num_vms_to_load}VMs\""
      ],
      "metadata": {
        "id": "Y4lrYzgxwbLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- Helper: Generate Run Folder Name ---------\n",
        "\n",
        "def get_run_folder_name(train_config, vm_count):\n",
        "    target = \"cpu\"\n",
        "    past = f\"past{train_config['max_encoder_length']}\"\n",
        "    fut = f\"fut{train_config['max_prediction_length']}\"\n",
        "    batch = f\"bs{train_config['batch_size']}\"\n",
        "    lr = f\"lr{train_config['learning_rate']:.0e}\".replace('+0', '')\n",
        "    hid = f\"hid{train_config['hidden_size']}\"\n",
        "\n",
        "    return f\"{target}_{vm_count}_{past}_{fut}_{batch}_{lr}_{hid}_{timestamp}\""
      ],
      "metadata": {
        "id": "bOkI1ts2yIlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpDH8N3TwmtZ"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Drop unused columns - place this here\n",
        "columns_to_keep = (\n",
        "    train_config[\"time_varying_known_reals\"]\n",
        "    + train_config[\"time_varying_unknown_reals\"]\n",
        "    + train_config[\"targets\"]\n",
        "    + train_config[\"group_ids\"]\n",
        "    + ['time_idx', 'timestamp']\n",
        ")\n",
        "columns_to_keep = list(set(columns_to_keep))  # Remove duplicates if any\n",
        "tft_df = tft_df[columns_to_keep]\n",
        "\n",
        "print(f\"‚úÖ Columns after filtering: {len(tft_df.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup output and log folders based on run config\n",
        "\n",
        "folder_name = get_run_folder_name(train_config, vm_count)\n",
        "\n",
        "train_config[\"output_base_dir\"] = os.path.join(train_config[\"output_base_dir\"], folder_name)\n",
        "train_config[\"log_dir\"] = os.path.join(train_config[\"log_dir\"], folder_name)\n",
        "\n",
        "os.makedirs(train_config[\"output_base_dir\"], exist_ok=True)\n",
        "os.makedirs(train_config[\"log_dir\"], exist_ok=True)\n",
        "\n",
        "print(\"Output directory:\", train_config[\"output_base_dir\"])\n",
        "print(\"Log directory:\", train_config[\"log_dir\"])\n",
        "\n",
        "# Now you can proceed to model training using `tft_df` and `train_config` as usual."
      ],
      "metadata": {
        "id": "Ib5yqZQz-9PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Basic Dataset Summary\n",
        "print(f\"\\n‚úÖ Dataset Summary:\")\n",
        "print(f\"- Min time_idx      : {tft_df['time_idx'].min()}\")\n",
        "print(f\"- Max time_idx      : {tft_df['time_idx'].max()}\")\n",
        "print(f\"- Total rows        : {len(tft_df):,}\")\n",
        "print(f\"- Batch size config : {train_config['batch_size']}\")\n",
        "\n",
        "# Train/Val Split Calculation\n",
        "max_time_idx = tft_df['time_idx'].max()\n",
        "split_point = max_time_idx * 0.8\n",
        "\n",
        "train_rows = (tft_df['time_idx'] <= split_point).sum()\n",
        "val_rows = (tft_df['time_idx'] > split_point).sum()\n",
        "total_rows = len(tft_df)\n",
        "val_pct = 100 * val_rows / total_rows\n",
        "\n",
        "print(f\"\\n‚úÖ Train/Validation Split:\")\n",
        "print(f\"- Split point (time_idx > {split_point:.2f})\")\n",
        "print(f\"- Train rows       : {train_rows:,}\")\n",
        "print(f\"- Validation rows  : {val_rows:,} ({val_pct:.2f}%)\")\n",
        "\n",
        "# Important for model creation\n",
        "val_df = tft_df[tft_df['time_idx'] > split_point]\n",
        "\n",
        "# Validation Window Stats\n",
        "val_points = len(val_df)\n",
        "min_required = train_config['max_encoder_length'] + train_config['max_prediction_length']\n",
        "total_val_windows = val_points - min_required + 1\n",
        "expected_val_batches = math.ceil(total_val_windows / train_config['batch_size'])\n",
        "\n",
        "print(f\"\\n‚úÖ Validation Window Stats:\")\n",
        "print(f\"- Validation points : {val_points:,}\")\n",
        "print(f\"- Min required points: {min_required}\")\n",
        "print(f\"- Total windows      : {total_val_windows:,}\")\n",
        "print(f\"- Expected batches   : {expected_val_batches:,}\")"
      ],
      "metadata": {
        "id": "7Z-9l7eg5-3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset index (important for unique indexing)\n",
        "tft_df = tft_df.reset_index(drop=True)\n",
        "\n",
        "# Prepare TimeSeriesDataSet for training portion (80%)\n",
        "dataset = TimeSeriesDataSet(\n",
        "    tft_df[tft_df.time_idx <= tft_df['time_idx'].max() * 0.8],\n",
        "    time_idx='time_idx',\n",
        "    target=train_config[\"targets\"][0],  # 'cpu_utilization_ratio' here\n",
        "    group_ids=train_config[\"group_ids\"],\n",
        "    max_encoder_length=train_config[\"max_encoder_length\"],\n",
        "    max_prediction_length=train_config[\"max_prediction_length\"],\n",
        "    time_varying_known_reals=train_config[\"time_varying_known_reals\"],\n",
        "    time_varying_unknown_reals=train_config[\"time_varying_unknown_reals\"]\n",
        "    target_normalizer=GroupNormalizer(groups=train_config[\"group_ids\"]),\n",
        "    add_relative_time_idx=True,\n",
        "    add_target_scales=True,\n",
        "    add_encoder_length=True,\n",
        "    allow_missing_timesteps=True\n",
        ")\n",
        "\n",
        "# Validation dataset for prediction (no randomization, full data)\n",
        "# ‚úÖ \"TimeSeriesDataSet applies sliding window logic on the training data,\n",
        "# using the full configuration like past steps, future steps, groups, and prepares the dataset accordingly.\"\n",
        "\n",
        "val_df = val_df.reset_index(drop=True) # Reset index of val_df\n",
        "\n",
        "val_dataset = TimeSeriesDataSet.from_dataset(\n",
        "    dataset, val_df, predict=True, stop_randomization=True\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "# performs Batching, Padding, Time-aware slicing for forecasting\n",
        "\n",
        "train_dataloader = dataset.to_dataloader(\n",
        "    train=True,\n",
        "    batch_size=train_config[\"batch_size\"],\n",
        "    num_workers=train_config[\"num_workers\"]\n",
        ")\n",
        "\n",
        "val_dataloader = val_dataset.to_dataloader(\n",
        "    train=False,\n",
        "    batch_size=train_config[\"batch_size\"],\n",
        "    num_workers=train_config[\"num_workers\"]\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"‚úÖ Dataset and dataloaders ready. Train batches: {len(train_dataloader)}, Val batches: {len(val_dataloader)}\")"
      ],
      "metadata": {
        "id": "Ra7_fpzplKsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7"
      ],
      "metadata": {
        "id": "ifn5CPwejuIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging & Callbacks\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "\n",
        "for target in train_config[\"targets\"]:\n",
        "    print(f\"\\nüîÅ Training for target: {target}\")\n",
        "\n",
        "    run_dir = os.path.join(train_config[\"output_base_dir\"], f\"{target}_run_{timestamp}\")\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "    # Save Raw cleaned DF for inspection, tft_df is pandas dataframe.\n",
        "    tft_df.to_csv(f\"{run_dir}/tft_df.csv\", index=False)\n",
        "\n",
        "    # Save structured TimeSeriesDataset for future reuse\n",
        "    dataset.save(f\"{run_dir}/tft_df\")\n",
        "\n",
        "    # ‚úÖ Save timestamp mapping if columns exist\n",
        "    timestamp_cols_needed = ['vm_id', 'timestamp', 'time_idx']\n",
        "    missing_cols = [col for col in timestamp_cols_needed if col not in val_df.columns]\n",
        "\n",
        "    if missing_cols:\n",
        "        print(f\"‚ö†Ô∏è Skipping timestamp mapping export. Columns missing: {missing_cols}\")\n",
        "    else:\n",
        "        timestamp_mapping_df = val_df[timestamp_cols_needed].reset_index(drop=True)\n",
        "        timestamp_mapping_df.to_csv(f\"{run_dir}/forecast_timestamp_mapping.csv\", index=False)\n",
        "        print(f\"‚úÖ Saved timestamp mapping to: {run_dir}/forecast_timestamp_mapping.csv\")\n",
        "\n",
        "    # Setup logging & checkpointing\n",
        "    logger = CSVLogger(save_dir=train_config[\"log_dir\"], name=f\"{target}_log\")\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor=\"val_loss\",\n",
        "        dirpath=run_dir,\n",
        "        filename=\"tft-{epoch:02d}-{val_loss:.2f}\",\n",
        "        save_top_k=1,\n",
        "        save_last=True,\n",
        "        mode=\"min\"\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        patience=train_config[\"early_stopping_patience\"],\n",
        "        mode=\"min\"\n",
        "    )"
      ],
      "metadata": {
        "id": "X07NZKn0SeUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8"
      ],
      "metadata": {
        "id": "U-V1Oi69jw5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, Lightning, Trainer"
      ],
      "metadata": {
        "id": "EUJRG3D76ISe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "\n",
        "class TFTLightningModule(pl.LightningModule):\n",
        "    def __init__(self, tft_model: TemporalFusionTransformer, learning_rate: float, loss_fn: torch.nn.Module):\n",
        "        super().__init__()\n",
        "        self.tft_model = tft_model\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.tft_model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.loss_fn(y_hat.prediction, y) # Extract prediction from output\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.loss_fn(y_hat.prediction, y) # Extract prediction from output\n",
        "        self.log(\"val_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "# Create the TFT model\n",
        "tft_model = TemporalFusionTransformer.from_dataset(\n",
        "    dataset,\n",
        "    learning_rate=train_config[\"learning_rate\"],\n",
        "    hidden_size=train_config[\"hidden_size\"],\n",
        "    dropout=train_config[\"dropout\"],\n",
        "    loss=train_config[\"loss_fn\"],\n",
        "    log_interval=10,\n",
        "    reduce_on_plateau_patience=4\n",
        ")\n",
        "\n",
        "# Wrap the TFT model in a LightningModule\n",
        "model = TFTLightningModule(\n",
        "    tft_model=tft_model,\n",
        "    learning_rate=train_config[\"learning_rate\"],\n",
        "    loss_fn=train_config[\"loss_fn\"]\n",
        ")\n",
        "\n",
        "# Setup Trainer\n",
        "if torch.cuda.is_available():\n",
        "    accelerator = \"gpu\"\n",
        "    devices = 1\n",
        "else:\n",
        "    accelerator = \"cpu\"\n",
        "    devices = 1\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=train_config[\"epochs\"],\n",
        "    accelerator=accelerator,\n",
        "    devices=devices,\n",
        "    logger=logger,\n",
        "    callbacks=[checkpoint_callback, early_stopping],\n",
        "    enable_checkpointing=True\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
      ],
      "metadata": {
        "id": "paex9VsEaJBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9"
      ],
      "metadata": {
        "id": "qWzxncyzjz2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç Make raw predictions on validation set"
      ],
      "metadata": {
        "id": "xKis-Xvt6PHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging & Callbacks\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "\n",
        "for target in train_config[\"targets\"]:\n",
        "    print(f\"\\nüîÅ Training for target: {target}\")\n",
        "\n",
        "    run_dir = os.path.join(train_config[\"output_base_dir\"], f\"{target}_run_{timestamp}\")\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "\n",
        "    # Save Raw cleaned DF for inspection (TFT input format)\n",
        "    tft_df.to_csv(f\"{run_dir}/tft_df.csv\", index=False)\n",
        "\n",
        "    # Save structured TimeSeriesDataset (structure, scalers, etc.)\n",
        "    dataset.save(f\"{run_dir}/tft_df_metadata\")  # <-- .save stores dataset metadata\n",
        "\n",
        "    if all(col in val_df.columns for col in meta_cols):\n",
        "        meta_df = val_df[meta_cols].reset_index(drop=True)\n",
        "        meta_df.to_csv(f\"{run_dir}/forecast_metadata.csv\", index=False)\n",
        "        print(f\"‚úÖ Metadata saved to: {run_dir}/forecast_metadata.csv\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Skipping metadata save ‚Äî columns not found: {meta_cols}\")\n",
        "\n",
        "    # Setup logging & checkpointing\n",
        "    logger = CSVLogger(save_dir=train_config[\"log_dir\"], name=f\"{target}_log\")\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor=\"val_loss\",\n",
        "        dirpath=run_dir,\n",
        "        filename=\"tft-{epoch:02d}-{val_loss:.2f}\",\n",
        "        save_top_k=1,\n",
        "        save_last=True,\n",
        "        mode=\"min\"\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        patience=train_config[\"early_stopping_patience\"],\n",
        "        mode=\"min\"\n",
        "    )"
      ],
      "metadata": {
        "id": "jKofCIO0aQEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicted CPU Utilization next future steps based on above config."
      ],
      "metadata": {
        "id": "cmpvQ087SI_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(forecast)\n",
        "print(type(forecast))\n",
        "print(forecast.shape)"
      ],
      "metadata": {
        "id": "gbhIzkOpIdkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10"
      ],
      "metadata": {
        "id": "Cmm5RNurj5Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spike Detection & Save Metadata"
      ],
      "metadata": {
        "id": "Gn6zSuWTnP7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç Simple spike detection based on 95th percentile threshold\n",
        "spikes = forecast > np.percentile(forecast, 95)\n",
        "\n",
        "# Save run notes and spike count\n",
        "with open(f\"{run_dir}/notes.txt\", \"w\") as f:\n",
        "    f.write(f\"Target: {target}\\n\")\n",
        "    f.write(f\"Spikes > 95th percentile: {int(spikes.sum())}\\n\")\n",
        "    f.write(\"Review plot.png and predictions.csv for further insights.\\n\")\n",
        "\n",
        "# üíæ Save training config for reproducibility\n",
        "# Create a serializable version of train_config\n",
        "serializable_train_config = train_config.copy()\n",
        "# Replace the non-serializable loss_fn object with its name\n",
        "serializable_train_config[\"loss_fn\"] = serializable_train_config[\"loss_fn\"].__class__.__name__\n",
        "\n",
        "with open(f\"{run_dir}/modelconfig.json\", \"w\") as f:\n",
        "    json.dump(serializable_train_config, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Run complete. Outputs saved at: {run_dir}\")"
      ],
      "metadata": {
        "id": "ZZViKR3poBY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_path = os.path.join(run_dir, \"predictions.csv\")\n",
        "print(f\"‚úÖ Forecast values saved to: {forecast_path}\")"
      ],
      "metadata": {
        "id": "5iELq2nboDno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Command to inspect .ckpt\n",
        "\n",
        "# ckpt = torch.load(\"path/to/tft-epoch=01-val_loss=0.04.ckpt\", map_location=torch.device('cpu'))\n",
        "# print(ckpt.keys())"
      ],
      "metadata": {
        "id": "rxsLstiSU8nn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(ckpt[\"epoch\"])             # Epoch number\n",
        "# print(ckpt[\"global_step\"])       # Total steps\n",
        "# print(ckpt[\"hyper_parameters\"])  # Saved hyperparameters"
      ],
      "metadata": {
        "id": "ebOmwcNBVDlH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2133a3972b28273c10fa027bbde5fb58efc69f3a1cd517826cf4b1affadfce4e"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}